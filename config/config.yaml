# XLSR-Transducer Configuration

# Data configuration
data:
  train_manifest: "data/train_list.txt"
  valid_manifest: "data/val_list.txt"
  test_manifest: "data/test_list.txt"
  audio_dir: ""
  sample_rate: 16000
  max_duration: 10.0  # Reduced from 30.0 to limit sequence lengths
  min_duration: 0.5
  batch_size: 24
  num_workers: 8

# Model configuration
model:
  # Encoder (XLSR-53) configuration
  encoder:
    pretrained_model_name: "facebook/wav2vec2-large-xlsr-53"
    freeze_feature_encoder: true
    freeze_base_model: false
    dropout: 0.0
    layerdrop: 0.0
    attention_mask_type: "chunk"  # Using chunked attention for streaming
    chunk_size: 32  # ~640ms as recommended in the paper (32 frames @ 20ms stride)
    left_context: 32  # One chunk of left context
    right_context: 0  # No future context for streaming capability
    attention_sink_size: 4  # Number of frames to use as attention sink
  
  # Predictor network configuration
  predictor:
    embedding_dim: 256
    hidden_dim: 640
    num_layers: 2
    dropout: 0.0
  
  # Joint network configuration
  joint:
    hidden_dim: 640
    activation: "tanh"

# Training configuration
training:
  num_epochs: 50
  learning_rate: 0.000005
  weight_decay: 0.000001
  warmup_steps: 5000
  grad_clip: 0.5
  save_dir: "checkpoints"
  log_dir: "logs"
  output_dir: "outputs"
  device: "cuda"
  log_interval: 100
  eval_interval: 1000
  save_interval: 5000
  early_stopping_patience: 5
  scheduler: "constant"  # Options: "linear", "cosine", "constant"
  use_fp16: false  # Enable mixed precision training for faster performance
  gradient_accumulation_steps: 4  # Process smaller batches but maintain effective batch size

# Tokenizer configuration
tokenizer:
  type: "bpe"  # Using BPE tokenizer for more efficient token sequences
  vocab_size: 1000
  # For character tokenizer, vocab_size is determined by the alphabet plus special tokens
  # For BPE tokenizer, you would set an explicit vocab_size (e.g., 1000-5000)
  # Estonian alphabet + digits + punctuation + special tokens requires ~60 tokens
  special_tokens:
    pad: "<pad>"
    unk: "<unk>"
    bos: "<s>"
    eos: "</s>"
    blank: "<blank>"

# Inference configuration
inference:
  beam_size: 5
  max_length: 100
  streaming_chunk_size: 32  # Match training chunk size
  streaming_buffer_size: 64  # Sufficient buffer for streaming 